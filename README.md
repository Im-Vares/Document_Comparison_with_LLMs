# Сравнение документов с помощью LLM

## Описание проекта
Этот проект демонстрирует несколько способов сравнения документов (шаблона и проверяемого текста) с использованием языковых моделей (LLM) через API Hugging Face. Основная цель – выявить, что в проверяемом документе **отсутствует**, что **изменено** и что **добавлено** по сравнению с эталоном.

## Структура файлов

- **direct_testing.py** – «прямой метод»: шаблон и прототип подаются LLM целиком, и модель сама выделяет отсутствующие, изменённые и лишние элементы.

- **quistions.py** – реализация «вопросно-ответного подхода»:
  1. Модель генерирует список проверочных вопросов по шаблону.  
  2. На эти вопросы проверяется прототип.  
  3. Ответы собираются в итоговый отчёт.

- **Two-step_comparison.py** – реализация метода «Двухшаговое сравнение»:
  1. Сначала проверяет, что из шаблона отсутствует или изменено.  
  2. Затем ищет, что лишнее в проверяемом документе.  

- **example_of_right.docx** – эталонный документ (шаблон).  
- **example_of_text.docx** – проверяемый документ (прототип).

## Как работает приложение
1. Через интерфейс Streamlit загружаются два документа (шаблон и проверяемый).
2. Выбирается метод проверки и модель (LLaMA, Qwen, Mistral и др.).
3. Отправляется запрос к Hugging Face API, модель анализирует тексты.
4. Возвращается отчёт в структурированном формате:
   - **ОТСУТСТВУЕТ** – что есть в шаблоне, но нет в проверяемом документе.
   - **ИЗМЕНЕНО / ЧАСТИЧНО** – что изменено по формулировке.
   - **ЛИШНЕЕ / ДОБАВЛЕНО** – что добавлено в проверяемом документе.

## Как запустить
1. Установите зависимости:
   ```bash
   pip install streamlit python-docx huggingface_hub
   ```
2. Установите токен Hugging Face:
   ```bash
   export HF_TOKEN=your_token_here
   ```
3. Запустите Streamlit:
   ```bash
   streamlit run Two-step_comparison.py
   ```
   (или другой файл метода — например, `direct_testing.py`).
